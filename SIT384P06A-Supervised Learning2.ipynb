{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical 6: Supervised Learning \n",
    "\n",
    "\n",
    "Upon completion of this session you should be able to:\n",
    "- understand how kNN, Naive Bayes, Decision Tree and Random Forest algorithms work.\n",
    "- be able to apply these supervised learning algorithms in Python.\n",
    "\n",
    "---\n",
    "- Materials in this module include resources collected from various open-source online repositories.\n",
    "- Jupyter source file can be downloaded from https://github.com/gaoshangdeakin/SIT384-Jupyter\n",
    "- If you found any issue/bug for this document, please submit an issue at [https://github.com/gaoshangdeakin/SIT384/issues](https://github.com/gaoshangdeakin/SIT384/issues)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "This practical session will demonstrate different supervised learning algorithms: kNN, Naive Bayes, Decision Tree and Random Forest.\n",
    "\n",
    "\n",
    "## Background\n",
    "\n",
    "\n",
    "### Part 1 K-NN Classification\n",
    "\n",
    "1.1 [K-NN in Python](#knn)\n",
    "\n",
    "1.2 [Decision Boundary](#db)\n",
    "\n",
    "### Part 2 Naive Bayes Classifier\n",
    "\n",
    "2.1 [NBC by Example](#nbc)\n",
    "\n",
    "2.2 [NBC Exercise](#nbc2)\n",
    "\n",
    "### Part 3 Random Forest\n",
    "\n",
    "3.1 [Decision Trees](#rf2)\n",
    "\n",
    "3.2 [Decision Trees and over-fitting](#rf3)\n",
    "\n",
    "3.3 [Ensembles of Estimators: Random Forests](#rf4)\n",
    "\n",
    "3.4 [Random Forest Regressor](#rf5)\n",
    "\n",
    "3.5 [Random Forest Limitations](#rf6)\n",
    "\n",
    "\n",
    "### Part 4 Confidence Interval\n",
    "\n",
    "4.1 [Population and Sample](#popsample)\n",
    "\n",
    "4.2 [Confidence Interval](#ci)\n",
    "\n",
    "## Tasks\n",
    "\n",
    "## Summary\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### <span style=\"color:#0b486b\">1. k-Nearest Neighbours Classification</span> \n",
    "\n",
    "kNN is a non-parametric classification technique which is extensively used in practice. Its input consists of the `k` closest training examples and the output is a class membership. An object is classified by a majority vote of its neighbors, with the object being assigned to the class most common among its `k` nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.\n",
    "\n",
    "**Please note that kNN is different from K-means.** K-means is a clustering algorithm that tries to partition a set of points into K sets (clusters) such that the points in each cluster be close to each other. It is unsupervised because the points have no external classification. kNN is a classification algorithm that in order to determine the classification of a point, combines the class of the k nearest points. It is supervised because you are trying to classify a point based on the known label of other points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"knn\"></a>\n",
    "\n",
    "### <span style=\"color:#0b486b\">1.1 kNN in Python</span> \n",
    "\n",
    "To be able to illustrate how we perform kNN classification in Python, we need some data first. Therefore we synthesize some data from 3 classes. We assume the data in each class comes from a multivariate random distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#seaborn: statistical data visualization. \n",
    "#Seaborn is a Python data visualization library based on matplotlib. \n",
    "#It provides a high-level interface for drawing attractive and informative statistical graphics.\n",
    "import seaborn as sns\n",
    "\n",
    "#%matplotlib inline\n",
    "\n",
    "#To switch to seaborn defaults, simply call the set() function.\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#Pseudo-random number generators work by performing some operation on a value. \n",
    "#Generally this value is the previous number generated by the generator. \n",
    "#However, the first time you use the generator, there is no previous value.\n",
    "#\n",
    "#Seeding a pseudo-random number generator gives it its first \"previous\" value. \n",
    "#Each seed value will correspond to a sequence of generated values for a given random number generator. \n",
    "#That is, if you provide the same seed twice, you get the same sequence of numbers twice.\n",
    "#\n",
    "#\n",
    "np.random.seed(100)\n",
    "\n",
    "n_per_class = 50\n",
    "colors = ['green', 'blue', 'magenta']\n",
    "\n",
    "mean1 = [-5, 10]\n",
    "cov1 = [[1.5, 0], [0, 1.5]]\n",
    "mean2 = [0, 7]\n",
    "cov2 = [[1.5, 0], [0, 3]]\n",
    "mean3 = [-6, 6]\n",
    "cov3 = [[2, 0], [0, 1.5]]\n",
    "\n",
    "means = [mean1, mean2, mean3]\n",
    "covs = [cov1, cov2, cov3]\n",
    "\n",
    "#\n",
    "#np.random.multivariate_normal()\n",
    "#Draw random samples from a multivariate normal distribution.\n",
    "#\n",
    "#The multivariate normal, multinormal or Gaussian distribution is \n",
    "#a generalization of the one-dimensional normal distribution to higher dimensions. \n",
    "#Such a distribution is specified by its mean and covariance matrix. \n",
    "#These parameters are analogous to the mean (average or “center”) \n",
    "#and variance (standard deviation, or “width,” squared) of the one-dimensional normal distribution.\n",
    "#For detail, go to https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.random.multivariate_normal.html \n",
    "#\n",
    "\n",
    "x11, x12 = np.random.multivariate_normal(means[0], covs[0], n_per_class).T\n",
    "x21, x22 = np.random.multivariate_normal(means[1], covs[1], n_per_class).T\n",
    "x31, x32 = np.random.multivariate_normal(means[2], covs[2], n_per_class).T\n",
    "\n",
    "scale = 75\n",
    "alpha = 0.6\n",
    "\n",
    "fig, ax  = plt.subplots(figsize=(7, 7), dpi=100)\n",
    "ax.scatter(x11, x12, alpha=alpha, color=colors[0], s=scale)\n",
    "ax.scatter(x21, x22, alpha=alpha, color=colors[1], s=scale)\n",
    "ax.scatter(x31, x32, alpha=alpha, color=colors[2], s=scale)\n",
    "\n",
    "ax.set_title(\"synthesized data for 3 classes\")\n",
    "ax.set_xlabel(\"$x_1$\")\n",
    "ax.set_ylabel(\"$x_2$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we have to instantiate a kNN classifier from sklearn. Below is how we define K and create kNN classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "\n",
    "weights='uniform'\n",
    "k = 15\n",
    "knn = neighbors.KNeighborsClassifier(k,weights=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use the above data to create the kNN classifier. We need to pass one array as training features and one array as training labels to the `knn` object. Therefore we have to put all the attributes together (also class labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#np.r_\n",
    "#This is used to concatenate any number of array slices along row (first) axis. \n",
    "#This is a simple way to create numpy arrays quickly and efficiently. \n",
    "#For detail, go to https://docs.scipy.org/doc/numpy/reference/generated/numpy.r_.html\n",
    "#\n",
    "#concatenate all the arraysalong row axis  \n",
    "x1 = np.r_[x11, x21, x31]\n",
    "x2 = np.r_[x12, x22, x32]\n",
    "#\n",
    "#np.c_\n",
    "#np.c_ is another way of doing array concatenate.\n",
    "#It translates slice objects to concatenation along the second axis.\n",
    "#e.g. >>> np.c_[np.array([[1,2,3]]), 0, 0, np.array([[4,5,6]])]\n",
    "#array([[1, 2, 3, 0, 0, 4, 5, 6]])\n",
    "#\n",
    "##concatenate all the arrays along column axis  \n",
    "X_train = np.c_[x1, x2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#general Y_train label output for the X_train input: 50 zero (class 0), 50 one (class 1) and 50 two (class 2)\n",
    "Y_train = np.r_[0*np.ones(n_per_class), 1*np.ones(n_per_class), 2*np.ones(n_per_class)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will see how kNN classifies a point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if set k=1, only one nearest neighbor\n",
    "k = 1\n",
    "knn = neighbors.KNeighborsClassifier(k)\n",
    "knn.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "cmap_bold = ListedColormap(['green', 'blue', 'magenta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(7, 7))\n",
    "\n",
    "#plot the classification of X_train\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], c=Y_train, cmap=cmap_bold, alpha=alpha, s=scale)\n",
    "\n",
    "plt.title(\"3-Class classification (k = {})\".format(k))\n",
    "\n",
    "#create an input to predict its label\n",
    "X_test = [[-7, 10]]\n",
    "Y_pred = knn.predict(X_test)\n",
    "\n",
    "#to check the predicted label for the X_test\n",
    "print(Y_pred)\n",
    "\n",
    "#plot the test input\n",
    "ax.scatter(X_test[0][0], X_test[0][1], marker=\"x\", s=scale, lw=2, c='k')\n",
    "#as you can see, the test input is displayed in color 'k'\n",
    "#how could it be colored based on the prediction Y_pred?\n",
    "\n",
    "ax.set_title(\"3-Class classification (k = {})\\n Red point is predicted as class {}\".format(k, colors[Y_pred.astype(int)[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"db\"></a>\n",
    "\n",
    "### <span style=\"color:#0b486b\">1.2 Decision Boundry</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kNN effectively partitions the feature space into different sets and assigns the same class label to points belonging to the same partition. This partitioning changes as we change k. We illustrate this below. As you see bigger values of k, partition the space more smoothly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 15\n",
    "knn = neighbors.KNeighborsClassifier(k, weights=weights)\n",
    "knn.fit(X_train, Y_train)\n",
    "\n",
    "# step size in the mesh\n",
    "h = 0.05\n",
    "\n",
    "# Create colour maps\n",
    "cmap_light = ListedColormap(['#AAFFAA', '#AAAAFF', '#FFAAAA'])\n",
    "cmap_bold = ListedColormap(['green', 'blue', 'magenta'])\n",
    "\n",
    "x1_min, x1_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1\n",
    "x2_min, x2_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1\n",
    "xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, h), np.arange(x2_min, x2_max, h))\n",
    "\n",
    "Z = knn.predict(np.c_[xx1.ravel(), xx2.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx1.shape)\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(7, 7))\n",
    "ax.pcolormesh(xx1, xx2, Z, cmap=cmap_light)\n",
    "\n",
    "# Plot also the training points\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], c=Y_train, cmap=cmap_bold, alpha=alpha, s=scale)\n",
    "\n",
    "plt.xlim(xx1.min(), xx1.max())\n",
    "plt.ylim(xx2.min(), xx2.max())\n",
    "plt.title(\"3-Class classification (k = {})\".format(k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will investigate the effect of `'k'` on decision boundaries. Lets train a classifier with `k=1` which means we only use the label of the closest point to predict the label of a test point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 1\n",
    "knn = neighbors.KNeighborsClassifier(k, weights=weights)\n",
    "knn.fit(X_train, Y_train)\n",
    "\n",
    "Z = knn.predict(np.c_[xx1.ravel(), xx2.ravel()])\n",
    "Z = Z.reshape(xx1.shape)\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(7, 7))\n",
    "ax.pcolormesh(xx1, xx2, Z, cmap=cmap_light)\n",
    "\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], c=Y_train, cmap=cmap_bold, alpha=alpha, s=scale)\n",
    "\n",
    "plt.xlim(xx1.min(), xx1.max())\n",
    "plt.ylim(xx2.min(), xx2.max())\n",
    "plt.title(\"3-Class classification (k = {})\".format(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 2\n",
    "knn = neighbors.KNeighborsClassifier(k, weights=weights)\n",
    "knn.fit(X_train, Y_train)\n",
    "\n",
    "Z = knn.predict(np.c_[xx1.ravel(), xx2.ravel()])\n",
    "Z = Z.reshape(xx1.shape)\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(7, 7))\n",
    "ax.pcolormesh(xx1, xx2, Z, cmap=cmap_light)\n",
    "\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], c=Y_train, cmap=cmap_bold, alpha=alpha, s=scale)\n",
    "\n",
    "plt.xlim(xx1.min(), xx1.max())\n",
    "plt.ylim(xx2.min(), xx2.max())\n",
    "plt.title(\"3-Class classification (k = {})\".format(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "knn = neighbors.KNeighborsClassifier(k, weights=weights)\n",
    "knn.fit(X_train, Y_train)\n",
    "\n",
    "Z = knn.predict(np.c_[xx1.ravel(), xx2.ravel()])\n",
    "Z = Z.reshape(xx1.shape)\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(7, 7))\n",
    "ax.pcolormesh(xx1, xx2, Z, cmap=cmap_light)\n",
    "\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], c=Y_train, cmap=cmap_bold, alpha=alpha, s=scale)\n",
    "\n",
    "plt.xlim(xx1.min(), xx1.max())\n",
    "plt.ylim(xx2.min(), xx2.max())\n",
    "plt.title(\"3-Class classification (k = {})\".format(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "knn = neighbors.KNeighborsClassifier(k, weights=weights)\n",
    "knn.fit(X_train, Y_train)\n",
    "\n",
    "Z = knn.predict(np.c_[xx1.ravel(), xx2.ravel()])\n",
    "Z = Z.reshape(xx1.shape)\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(7, 7))\n",
    "ax.pcolormesh(xx1, xx2, Z, cmap=cmap_light)\n",
    "\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], c=Y_train, cmap=cmap_bold, alpha=alpha, s=scale)\n",
    "\n",
    "plt.xlim(xx1.min(), xx1.max())\n",
    "plt.ylim(xx2.min(), xx2.max())\n",
    "plt.title(\"3-Class classification (k = {})\".format(k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Prediction\n",
    "\n",
    "Play with the `X_test` and `k` to see how the classifier behaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "knn = neighbors.KNeighborsClassifier(k, weights=weights)\n",
    "knn.fit(X_train, Y_train)\n",
    "\n",
    "Z = knn.predict(np.c_[xx1.ravel(), xx2.ravel()])\n",
    "Z = Z.reshape(xx1.shape)\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(7, 7))\n",
    "ax.pcolormesh(xx1, xx2, Z, cmap=cmap_light)\n",
    "\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], c=Y_train, cmap=cmap_bold, alpha=alpha, s=scale)\n",
    "\n",
    "plt.xlim(xx1.min(), xx1.max())\n",
    "plt.ylim(xx2.min(), xx2.max())\n",
    "plt.title(\"3-Class classification (k = {})\".format(k))\n",
    "\n",
    "X_test = [[-4, 8]]\n",
    "Y_pred = knn.predict(X_test)\n",
    "ax.scatter(X_test[0][0], X_test[0][1], alpha=0.95, color='r', s=3*scale)\n",
    "\n",
    "ax.set_title(\"3-Class classification (k = {})\\n Red point is predicted as class {}\".format(k, colors[Y_pred.astype(int)[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now instead of predicting the class label for one point, we use our model to predict the labels of multiple points.\n",
    "\n",
    "First we generate some test data from the first class. This way we know the true class labels. Then we can use the `kNN` classifier to predict labels for the test data and get the predicted class labels. A measure of  accuracy for the classifier can be defined by comparing the true and predicted labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test = 100\n",
    "X1_test, X2_test = np.random.multivariate_normal(mean1, cov1, n_test).T\n",
    "Y_true = 0 * np.ones(n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.c_[X1_test, X2_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many times the classifier predicts the labels correctly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred == Y_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(sum(Y_pred == Y_true) + 0.0) / n_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Repeat the previous experiment with a classifier which has been trained with a different `k`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">2. Naive Bayes Classifier</span> \n",
    "\n",
    "\n",
    "Naive Bayes is one of the most practical classification machine learning algorithms. It is  based on Bayes’ Theorem with an assumption of independence among predictors. It is: \n",
    "\n",
    "* fast\n",
    "* good performance\n",
    "* simple yet very effective\n",
    "* robust to irrelative features\n",
    "\n",
    "So why is it called naive?\n",
    "\n",
    "Because it does not consider the dependency between features and assume all features are independent of each other which is not the case in reality. This is a naive assumption, hence the name.\n",
    "\n",
    "Naive Bayes model is easy to build and particularly useful for very large data sets. Along with simplicity, Naive Bayes is known to outperform even highly sophisticated classification methods. There are many interesting tutorials explaining the Bayes' Theorem for beginners via google, eg. [https://dzone.com/articles/naive-bayes-tutorial-naive-bayes-classifier-in-pyt](https://dzone.com/articles/naive-bayes-tutorial-naive-bayes-classifier-in-pyt). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is very good although this naive assumption. A famous example of NB usage is spam filtering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"nbc\"></a>\n",
    "### <span style=\"color:#0b486b\">2.1 NBC by Example</span> \n",
    "\n",
    "We assume we have collected the below data for the past 5 days. Based on this data, can we predict if our subject will play in a setting like:\n",
    "\n",
    "    outlook  = overcast\n",
    "    temp     = hot\n",
    "    humidity = normal\n",
    "    windy    = no"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- <img src=\"nb_data.png\" width=\"800\"> -->\n",
    "<img src=\"./image/nb_data.png\" width=\"800\">\n",
    "<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we have to find a representation for our data. We can construct a dictionary to convert stings into numbers and then save them in a dataframe. \n",
    "\n",
    "    outlook: sunny=0, overcast=1, rainy=2\n",
    "    temp: hot=0, mild=1, cool=2\n",
    "    humidity: normal=0, high=1\n",
    "    wind: no=0, yes=1\n",
    "    play: no=0, yes=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'outlook': [0, 1, 2, 0, 1],\n",
    "    'temp'   : [0, 1, 2, 1, 0],\n",
    "    'humid'  : [0, 0, 1, 0, 1],\n",
    "    'wind'   : [0, 0, 1, 1, 0],\n",
    "    'play'   : [1, 1, 0, 0, 0,]    \n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use Bayes rule to construct a Naive Bayes classifier. We can write:\n",
    "\n",
    "$$Pr\\left(p|o,t,h,w\\right)\\propto Pr\\left(p\\right)Pr(o|p)Pr(t|p)Pr(h|p)Pr(w|p)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate $Pr(p)$ we use marginal probablity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def marginal_prob(df, col):\n",
    "    ll = [(ss, (df[col] == ss).sum()) for ss in set(df[col])]\n",
    "    total_count = [b for a,b in ll]\n",
    "    total_count = sum(total_count)\n",
    "    \n",
    "    ll2 = [(a, b/total_count) for a, b in ll]\n",
    "    return dict(ll2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate probability of a feature given the class (play) we use conditinoal probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_prob(df, f, c, val):\n",
    "    df2 = df[df[c] == val][f]\n",
    "    ll = [[ss, (df2 == ss).sum()] for ss in set(df2)]\n",
    "    total_count = [b for a,b in ll]\n",
    "    total_count = sum(total_count)\n",
    "    \n",
    "    ll2 = [(a, b/total_count) for a, b in ll]\n",
    "    return dict(ll2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use Bayes rule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = 1\n",
    "t = 0\n",
    "h = 0\n",
    "w = 0\n",
    "\n",
    "c = 0\n",
    "p0 = marginal_prob(df, 'play')[c] * conditional_prob(df, 'outlook', 'play', c)[o] * conditional_prob(df, 'temp', 'play', c)[t] \\\n",
    "* conditional_prob(df, 'humid', 'play', c)[h] * conditional_prob(df, 'wind', 'play', c)[w]\n",
    "\n",
    "c = 1\n",
    "p1 = marginal_prob(df, 'play')[c] * conditional_prob(df, 'outlook', 'play', c)[o] * conditional_prob(df, 'temp', 'play', c)[t] \\\n",
    "* conditional_prob(df, 'humid', 'play', c)[h] * conditional_prob(df, 'wind', 'play', c)[w]\n",
    "\n",
    "# normalizing\n",
    "p_sum = p0 + p1\n",
    "p0 /= p_sum\n",
    "p1 /= p_sum\n",
    "\n",
    "print (\"probability of not playing: {}\".format(p0))\n",
    "print (\"probability of playing    : {}\".format(p1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"nbc2\"></a>\n",
    "### <span style=\"color:#0b486b\">2.2 NBC Exercise</span> \n",
    "\n",
    "\n",
    "\n",
    "Suppose we have documents below as our training set. \n",
    "\n",
    "    d1: Chinese Beijing Chinese , class = C\n",
    "    d2: Chinese Chinese Shanghai, class = C\n",
    "    d3: Chinese Macao           , class = C\n",
    "    d4: Tokyo Japan Chinese     , class = J\n",
    "\n",
    "\n",
    "Train a NB classifier and predict if `d5` belongs to class C or J.\n",
    "\n",
    "    d5: Chinese Chinese Chinese Tokyo Japan, class = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"rf1\"></a>\n",
    "\n",
    "## 3. Random Forest\n",
    "\n",
    "Random forests are an example of an *ensemble learner* built on decision trees.\n",
    "For this reason we'll start by discussing decision trees themselves.\n",
    "\n",
    "Decision trees are extremely intuitive ways to classify or label objects: you simply ask a series of questions designed to zero-in on the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn; \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy import stats\n",
    "import pylab as pl\n",
    "\n",
    "seaborn.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download [fig_code.zip](fig_code.zip) from week6 folder on Clouddeakin or [https://github.com/gaoshangdeakin/SIT384-libs/](https://github.com/gaoshangdeakin/SIT384-libs/). Extract and save the fig_code folder in the folder where your script is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fig_code\n",
    "fig_code.plot_example_decision_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The binary splitting makes this extremely efficient.\n",
    "As always, though, the trick is to *ask the right questions*.\n",
    "This is where the algorithmic process comes in: in training a decision tree classifier, the algorithm looks at the features and decides which questions (or \"splits\") contain the most information.\n",
    "\n",
    "<a id = \"rf2\"></a>\n",
    "\n",
    "### 3.1 Creating a Decision Tree\n",
    "\n",
    "Here's an example of a decision tree classifier in scikit-learn. We'll start by defining some two-dimensional labeled data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_blobs(n_samples=300, centers=4,\n",
    "                  random_state=0, cluster_std=1.0)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='rainbow');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have some convenience functions in the repository that help \n",
    "from fig_code import visualize_tree, plot_tree_interactive\n",
    "\n",
    "# Now using IPython's ``interact`` (available in IPython 2.0+, and requires a live kernel) we can view the decision tree splits:\n",
    "plot_tree_interactive(X, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that at each increase in depth, every node is split in two **except** those nodes which contain only a single class.\n",
    "The result is a very fast **non-parametric** classification, and can be extremely useful in practice.\n",
    "\n",
    "**Question: Do you see any problems with this?**\n",
    "\n",
    "<a id = \"rf3\"></a>\n",
    "\n",
    "### 3.2 Decision Trees and over-fitting\n",
    "\n",
    "One issue with decision trees is that it is very easy to create trees which **over-fit** the data. That is, they are flexible enough that they can learn the structure of the noise in the data rather than the signal! For example, take a look at two trees built on two subsets of this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "plt.figure()\n",
    "visualize_tree(clf, X[:200], y[:200], boundaries=False)\n",
    "plt.figure()\n",
    "visualize_tree(clf, X[-200:], y[-200:], boundaries=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The details of the classifications are completely different! That is an indication of **over-fitting**: when you predict the value for a new point, the result is more reflective of the noise in the model rather than the signal.\n",
    "\n",
    "<a id = \"rf4\"></a>\n",
    "\n",
    "### 3.3 Ensembles of Estimators: Random Forests\n",
    "\n",
    "One possible way to address over-fitting is to use an **Ensemble Method**: this is a meta-estimator which essentially averages the results of many individual estimators which over-fit the data. Somewhat surprisingly, the resulting estimates are much more robust and accurate than the individual estimates which make them up!\n",
    "\n",
    "One of the most common ensemble methods is the **Random Forest**, in which the ensemble is made up of many decision trees which are in some way perturbed.\n",
    "\n",
    "There are volumes of theory and precedent about how to randomize these trees, but as an example, let's imagine an ensemble of estimators fit on subsets of the data. We can get an idea of what these might look like as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_randomized_tree(random_state=0):\n",
    "    X, y = make_blobs(n_samples=300, centers=4,\n",
    "                      random_state=0, cluster_std=2.0)\n",
    "    clf = DecisionTreeClassifier(max_depth=15)\n",
    "    \n",
    "    rng = np.random.RandomState(random_state)\n",
    "    i = np.arange(len(y))\n",
    "    rng.shuffle(i)\n",
    "    visualize_tree(clf, X[i[:250]], y[i[:250]], boundaries=False,\n",
    "                   xlim=(X[:, 0].min(), X[:, 0].max()),\n",
    "                   ylim=(X[:, 1].min(), X[:, 1].max()))\n",
    "    \n",
    "from IPython.html.widgets import interact\n",
    "interact(fit_randomized_tree, random_state=[0, 100]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how the details of the model change as a function of the sample, while the larger characteristics remain the same!\n",
    "The random forest classifier will do something similar to this, but use a combined version of all these trees to arrive at a final answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=0, n_jobs=-1)\n",
    "visualize_tree(clf, X, y, boundaries=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By averaging over 100 randomly perturbed models, we end up with an overall model which is a much better fit to our data!\n",
    "\n",
    "*(Note: above we randomized the model through sub-sampling... Random Forests use more sophisticated means of randomization, which you can read about in, e.g. the [scikit-learn documentation](http://scikit-learn.org/stable/modules/ensemble.html#forest)*)\n",
    "\n",
    "<a id = \"rf5\"></a>\n",
    "\n",
    "### 3.4 Random Forest Regressor\n",
    "\n",
    "Above we were considering random forests within the context of classification.\n",
    "Random forests can also be made to work in the case of regression (that is, continuous rather than categorical variables). The estimator to use for this is ``sklearn.ensemble.RandomForestRegressor``.\n",
    "\n",
    "Let's quickly demonstrate how this can be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "x = 10 * np.random.rand(100)\n",
    "\n",
    "def model(x, sigma=0.3):\n",
    "    fast_oscillation = np.sin(5 * x)\n",
    "    slow_oscillation = np.sin(0.5 * x)\n",
    "    noise = sigma * np.random.randn(len(x))\n",
    "\n",
    "    return slow_oscillation + fast_oscillation + noise\n",
    "\n",
    "y = model(x)\n",
    "plt.errorbar(x, y, 0.3, fmt='o');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xfit = np.linspace(0, 10, 1000)\n",
    "yfit = RandomForestRegressor(100).fit(x[:, None], y).predict(xfit[:, None])\n",
    "ytrue = model(xfit, 0)\n",
    "\n",
    "plt.errorbar(x, y, 0.3, fmt='o')\n",
    "plt.plot(xfit, yfit, '-r');\n",
    "plt.plot(xfit, ytrue, '-k', alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the non-parametric random forest model is flexible enough to fit the multi-period data, without us even specifying a multi-period model!\n",
    "\n",
    "Tradeoff between simplicity and thinking about what your data is.\n",
    "\n",
    "Feature engineering is important, need to know your domain: Fourier transform frequency distribution.\n",
    "\n",
    "<a id = \"rf6\"></a>\n",
    "\n",
    "### 3.5 Random Forest Limitations\n",
    "\n",
    "The following data scenarios are not well suited for random forests:\n",
    "* y: lots of 0, few 1\n",
    "* Structured data like images where a neural network might be better\n",
    "* Small data size which might lead to overfitting\n",
    "* High dimensional data where a linear model might work better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <span style=\"color:#0b486b\">4. Confidence Interval</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"popsample\"></a>\n",
    "\n",
    "### <span style=\"color:#0b486b\">4.1 Population vs Sample</span> \n",
    "\n",
    "The main difference between population and sample comes down to how observations are assigned to dataset. A **population** includes all of the elements from the dataset. A **sample** consists of one or more observations from the population. In other words **population** is the entire collection of the desired measurable characteristic that we would have, if we could collect it. \n",
    "\n",
    "For example if suppose we want to find the average height of 2nd grade students in Australia. The population would be all the students who are studying in 2nd grade in Australia. But most probably we can not measure the height of all Australian 2nd grade students. It is not feasible. So what do we do? **We sample!**. We collect the height of some of Australian 2nd graders and based on that, we **estimate** the average height of the population (all of Australian 2nd grade students). The sample could be 2nd grade students of one class in one school, or multiple classes in multiple schools in one state, or  multiple schools in multiple states, or etc.\n",
    "\n",
    "A measurable statistic of a population (such as mean) is called a **parameter**. But a measurable characteristic of a sample is called **statistic**.\n",
    "\n",
    "----\n",
    "<a id = \"ci\"></a> \n",
    "\n",
    "### <span style=\"color:#0b486b\">4.2 Confidence Interval</span> \n",
    "\n",
    "As stated in previous section, population parameter is unknown. Confidence interval is a type of interval estimate of a population parameter calculated from sample statistics. It is an interval estimate combined with a probability.\n",
    "\n",
    "For the aforementioned example, it means that without collecting the height of all Aussie 2nd graders, we can estimate the average height by collecting a sample and using the below formula:\n",
    "\n",
    "\n",
    "$$Confidence\\, Interval=\\bar{X}\\pm z\\frac{s}{\\sqrt{n}}$$\n",
    "\n",
    "$s$ is the sample standard deviation, $n$ is the sample size, and $z$ is often read from a table.\n",
    "\n",
    "    confidence level (%)     z\n",
    "         \n",
    "            70              1.04 \n",
    "            75              1.15\n",
    "            80              1.28 \n",
    "            85              1.44 \n",
    "            90              1.645\n",
    "            92              1.75\n",
    "            95              1.96\n",
    "            96              2.05\n",
    "            98              2.33\n",
    "            99              2.58"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets use an example to clarify this concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as ss\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create a population. Please note that we do not use the population in our computations. We use samples from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma = 10, 2\n",
    "n_population = 10000\n",
    "population = np.random.normal(mu, sigma, n_population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.hist(population, bins=25)\n",
    "ax.set_title(r\"population histogram, $\\mu={}$, $\\sigma={}$ \".format(mu, sigma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we sample multiple times from this population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = 1000\n",
    "sample_size = 500\n",
    "samples = np.zeros([n_trials, sample_size])\n",
    "sample_std = np.zeros(n_trials)\n",
    "sample_mean = np.zeros(n_trials)\n",
    "\n",
    "for i in range(n_trials):\n",
    "    samples[i] = np.random.choice(population, size=sample_size, replace=False)\n",
    "    sample_mean[i] = samples[i].mean()\n",
    "    sample_std[i] = samples[i].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 200\n",
    "se = sample_std[i] / np.sqrt(sample_size)\n",
    "dist = ss.distributions.norm(sample_mean[i], se)\n",
    "z = 1.96\n",
    "\n",
    "x = np.linspace(9.5, 10.5, 100)\n",
    "y = dist.pdf(x)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "    \n",
    "ax.hist(sample_mean, normed=True, bins=20, label='sample means')\n",
    "ax.plot(x, y, label=\"$N(\\mu_{i},se)$\")\n",
    "ax.vlines(sample_mean[i] + z*se, 0, 5, label='$\\mu_i \\pm z\\sigma_i$')\n",
    "ax.vlines(sample_mean[i] - z*se, 0, 5)\n",
    "\n",
    "ax.set_title(\"distribution of sample statistics\\ni={}\".format(i))\n",
    "ax.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#0b486b\">Tasks</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the provided examples and get yourself familiar with sample code before attempting portolio tasks.\n",
    "\n",
    "Please show your attempt to your tutor before you leave the lab, or email your files to your coordinator if you are an off-campus student."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#0b486b\">Summary</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this session we have covered: \n",
    " - different supervised learning algorithms.\n",
    " - how to apply the supervised learning algorithms in Python."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
